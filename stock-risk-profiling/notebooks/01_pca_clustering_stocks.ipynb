{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4affde",
   "metadata": {},
   "source": [
    "# Stock Risk Profiling with PCA and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sns.set(context='notebook', style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30710c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = [\n",
    "    'PETR4.SA', 'VALE3.SA', 'ITUB4.SA',  # Brazil\n",
    "    'AAPL', 'MSFT', 'GOOGL',             # USA\n",
    "    'MC.PA', 'NESN.SW'                   # Europe\n",
    "]\n",
    "START = '2020-01-01'\n",
    "END = None\n",
    "PERIODS_PER_YEAR = 252\n",
    "RISK_FREE_PER_PERIOD = 0.0\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_drawdown(return_series: pd.Series) -> float:\n",
    "    wealth = (1 + return_series.fillna(0)).cumprod()\n",
    "    running_max = wealth.cummax()\n",
    "    drawdown = (wealth / running_max) - 1.0\n",
    "    return float(drawdown.min())\n",
    "\n",
    "def downside_deviation(returns: pd.Series, mar: float = 0.0) -> float:\n",
    "    downside = np.minimum(0, returns - mar)\n",
    "    return float(np.sqrt((downside ** 2).mean()))\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, rf: float = 0.0, periods_per_year: int = 252) -> float:\n",
    "    excess = returns - rf\n",
    "    mean = excess.mean() * periods_per_year\n",
    "    vol = excess.std(ddof=1) * np.sqrt(periods_per_year)\n",
    "    return float(np.nan) if vol == 0 else float(mean / vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae22e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = yf.download(TICKERS, start=START, end=END, auto_adjust=True)['Close']\n",
    "prices = prices.dropna(how='all')\n",
    "returns = prices.pct_change().dropna(how='all')\n",
    "\n",
    "features = {}\n",
    "for col in returns.columns:\n",
    "    r = returns[col].dropna()\n",
    "    if r.empty:\n",
    "        continue\n",
    "    mean_daily = r.mean()\n",
    "    vol_daily = r.std(ddof=1)\n",
    "    dd = compute_drawdown(r)\n",
    "    ddv = downside_deviation(r, mar=0.0)\n",
    "    shrp = sharpe_ratio(r, rf=RISK_FREE_PER_PERIOD, periods_per_year=PERIODS_PER_YEAR)\n",
    "    features[col] = {\n",
    "        'ret_daily_mean': mean_daily,\n",
    "        'ret_ann_mean': mean_daily * PERIODS_PER_YEAR,\n",
    "        'vol_daily': vol_daily,\n",
    "        'vol_ann': vol_daily * math.sqrt(PERIODS_PER_YEAR),\n",
    "        'max_drawdown': dd,\n",
    "        'downside_dev': ddv,\n",
    "        'sharpe_ann': shrp,\n",
    "        'n_obs': len(r)\n",
    "    }\n",
    "\n",
    "feat_df = pd.DataFrame(features).T.dropna().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feat_df.select_dtypes(include=[np.number]).copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=min(5, X.shape[1]))\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    'Explained_Variance_Ratio': pca.explained_variance_ratio_,\n",
    "    'Cumulative': np.cumsum(pca.explained_variance_ratio_)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = {}\n",
    "max_k = min(10, len(feat_df))\n",
    "for k in range(2, max_k):\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init='auto')\n",
    "    labels = km.fit_predict(X_pca[:, :2])\n",
    "    sil_scores[k] = silhouette_score(X_pca[:, :2], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sil_df = pd.DataFrame({'k': list(sil_scores.keys()), 'silhouette': list(sil_scores.values())}).sort_values('k')\n",
    "best_k = int(sil_df.loc[sil_df['silhouette'].idxmax(), 'k']) if not sil_df.empty else 3\n",
    "\n",
    "km = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init='auto')\n",
    "labels_km = km.fit_predict(X_pca[:, :2])\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=best_k)\n",
    "labels_agg = agg.fit_predict(X_pca[:, :2])\n",
    "\n",
    "result = feat_df.copy()\n",
    "result['PC1'] = X_pca[:, 0]\n",
    "result['PC2'] = X_pca[:, 1]\n",
    "result['cluster_km'] = labels_km\n",
    "result['cluster_agg'] = labels_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f50b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1),\n",
    "         pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Scree Plot – Explained Variance per Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sil_df['k'], sil_df['silhouette'], marker='o')\n",
    "plt.title('Silhouette Score by k (K-Means)')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA scatter with clusters\n",
    "plt.figure(figsize=(6,5))\n",
    "palette = sns.color_palette(n_colors=best_k)\n",
    "for cl in sorted(np.unique(labels_km)):\n",
    "    sel = labels_km == cl\n",
    "    plt.scatter(result.loc[sel, 'PC1'], result.loc[sel, 'PC2'], label=f'Cluster {cl}', s=60)\n",
    "for t in result.index:\n",
    "    plt.annotate(t, (result.loc[t, 'PC1'], result.loc[t, 'PC2']), fontsize=8, xytext=(4,4), textcoords='offset points')\n",
    "plt.title('PC1 vs PC2 – K-Means Clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save tabular results locally (not required for rendering on GitHub)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "result.to_csv('../results/pca_clustering_results.csv', index=True)\n",
    "print('Saved: ../results/pca_clustering_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
